{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmcODVLunVvoZpAAY0osIs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmcreek/Multi-Task-Learning/blob/main/encoder_and_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "b3nZWumclVWn",
        "outputId": "8f3cc141-8f0d-4715-cc91-dec2ff249e80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f1ed357-73f2-426a-b5a9-d6354683575c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f1ed357-73f2-426a-b5a9-d6354683575c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving val.parquet to val.parquet\n",
            "Saving train.parquet to train.parquet\n",
            "Saving classifier_dataset.csv to classifier_dataset.csv\n",
            "Saving ssl_dataset.csv to ssl_dataset.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xee in position 20: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1268325371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 3: Read parquet file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 4: Inspect structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xee in position 20: invalid continuation byte"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload parquet file\n",
        "uploaded = files.upload()  # this opens a dialog to choose your .parquet file\n",
        "\n",
        "# Step 2: Get filename (first key of uploaded dict)\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Step 3: Read parquet file into a DataFrame\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "# Step 4: Inspect structure\n",
        "print(df.head())   # show first few rows\n",
        "print(df.info())   # show column names + data types\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "BPbDnU-SD9Lc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AffectNet features (CSV)\n",
        "affectnet_df = pd.read_csv(\"ssl_dataset.csv\", low_memory=False)\n",
        "\n",
        "# DAPPER physiological features (Parquet)\n",
        "dapper_df = pd.read_parquet(\"train.parquet\")\n",
        "\n",
        "print(\"AffectNet shape:\", affectnet_df.shape)\n",
        "print(\"DAPPER shape:\", dapper_df.shape)\n",
        "print(\"AffectNet dtypes (sample):\\n\", affectnet_df.dtypes.head())\n",
        "print(\"DAPPER dtypes (sample):\\n\", dapper_df.dtypes.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rZ1neAD-Ts",
        "outputId": "8b1b7e06-22f5-48a1-9885-1915bf537860"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AffectNet shape: (121, 718)\n",
            "DAPPER shape: (341248, 63)\n",
            "AffectNet dtypes (sample):\n",
            " frame           int64\n",
            "face_id         int64\n",
            "timestamp     float64\n",
            "confidence    float64\n",
            "success         int64\n",
            "dtype: object\n",
            "DAPPER dtypes (sample):\n",
            " participant_id      int64\n",
            "window_id           int64\n",
            "start_time          int64\n",
            "center_time       float64\n",
            "hr_mean           float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainDataset(Dataset):\n",
        "    def __init__(self, dapper_df, affectnet_df):\n",
        "        # Work on copies to avoid modifying original dfs\n",
        "        dapper = dapper_df.copy()\n",
        "        affectnet = affectnet_df.copy()\n",
        "\n",
        "        # Drop known label/id columns (if present)\n",
        "        dapper = dapper.drop(columns=[\"participant_id\", \"window_id\", \"start_time\", \"center_time\", \"valence\", \"arousal\", \"panas_pos\", \"panas_neg\"], errors=\"ignore\")\n",
        "        affectnet = affectnet.drop(columns=[\"frame\", \"face_id\", \"timestamp\", \"emotion\", \"stress\", \"valence\", \"arousal\", \"confidence\", \"success\"], errors=\"ignore\")\n",
        "\n",
        "        # Keep only numeric columns (this drops object columns)\n",
        "        dapper = dapper.select_dtypes(include=[np.number])\n",
        "        affectnet = affectnet.select_dtypes(include=[np.number])\n",
        "\n",
        "        # Drop columns that are entirely NaN (like skew/kurtosis columns in your DAPPER preview)\n",
        "        dapper = dapper.dropna(axis=1, how=\"all\")\n",
        "        affectnet = affectnet.dropna(axis=1, how=\"all\")\n",
        "\n",
        "        # Fill remaining NaNs (choose mean or 0 — here we use column mean, fallback to 0)\n",
        "        dapper = dapper.fillna(dapper.mean()).fillna(0.0)\n",
        "        affectnet = affectnet.fillna(affectnet.mean()).fillna(0.0)\n",
        "\n",
        "        # Convert to float32\n",
        "        self.dapper = dapper.values.astype(np.float32)\n",
        "        self.affectnet = affectnet.values.astype(np.float32)\n",
        "\n",
        "        # Keep feature dims for later use\n",
        "        self.dapper_dim = self.dapper.shape[1]\n",
        "        self.affectnet_dim = self.affectnet.shape[1]\n",
        "\n",
        "        # Align lengths: cut to the shorter dataset (simple strategy for paired SSL)\n",
        "        min_len = min(len(self.dapper), len(self.affectnet))\n",
        "        self.dapper = self.dapper[:min_len]\n",
        "        self.affectnet = self.affectnet[:min_len]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dapper)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns (physio_tensor, image_features_tensor)\n",
        "        d = torch.from_numpy(self.dapper[idx])\n",
        "        a = torch.from_numpy(self.affectnet[idx])\n",
        "        return d, a\n",
        "\n",
        "# instantiate loader\n",
        "pretrain_dataset = PretrainDataset(dapper_df, affectnet_df)\n",
        "pretrain_loader = DataLoader(pretrain_dataset, batch_size=64, shuffle=True, drop_last=False)\n",
        "\n",
        "# quick sanity prints\n",
        "print(\"Pretrain dataset length:\", len(pretrain_dataset))\n",
        "print(\"DAPPER feature dim:\", pretrain_dataset.dapper_dim)\n",
        "print(\"AffectNet feature dim:\", pretrain_dataset.affectnet_dim)\n"
      ],
      "metadata": {
        "id": "Y77XIz4dEM_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31617bfa-ea02-4160-f586-e2bad69fc259"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain dataset length: 121\n",
            "DAPPER feature dim: 55\n",
            "AffectNet feature dim: 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PhysiologicalEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128):\n",
        "        \"\"\"\n",
        "        input_dim: number of features per window (treated as sequence length for 1D conv)\n",
        "        embed_dim: final embedding size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # We'll treat the features as a 1D 'signal' of length=input_dim with 1 channel\n",
        "        self.input_dim = input_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),  # halves length\n",
        "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)  # collapse length -> 1\n",
        "        )\n",
        "        self.fc = nn.Linear(64, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_dim) float tensor\n",
        "        returns: (batch, embed_dim)\n",
        "        \"\"\"\n",
        "        # ensure float and correct shape\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(\"Physio encoder expects input shape (batch, features)\")\n",
        "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
        "        x = self.encoder(x).squeeze(-1)  # (batch, 64)\n",
        "        out = self.fc(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Hsr0PxidEPaS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128):\n",
        "        \"\"\"\n",
        "        A simple MLP encoder for precomputed image features (AffectNet row features)\n",
        "        input_dim: number of feature columns from affectnet after preprocessing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, input_dim)\n",
        "        return self.encoder(x)\n"
      ],
      "metadata": {
        "id": "myXPjO9FERuk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 5 – Contrastive Pretraining (InfoNCE loss)\n",
        "# =========================\n",
        "def contrastive_loss(z_physio, z_image, temperature=0.1):\n",
        "    # Normalize embeddings\n",
        "    z_physio = F.normalize(z_physio, dim=1)\n",
        "    z_image  = F.normalize(z_image, dim=1)\n",
        "\n",
        "    # Similarity matrix\n",
        "    logits = torch.matmul(z_physio, z_image.T) / temperature\n",
        "    labels = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "    # Loss physio->image\n",
        "    loss_i = F.cross_entropy(logits, labels)\n",
        "\n",
        "    # Loss image->physio (transpose logits)\n",
        "    loss_t = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    return (loss_i + loss_t) / 2.0\n"
      ],
      "metadata": {
        "id": "jbdqSuIXE-FD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainModel(nn.Module):\n",
        "    def __init__(self, physio_encoder, image_encoder):\n",
        "        super().__init__()\n",
        "        self.physio_encoder = physio_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "\n",
        "    def forward(self, physio_x, image_x):\n",
        "        return self.physio_encoder(physio_x), self.image_encoder(image_x)\n"
      ],
      "metadata": {
        "id": "WQGteQA-E_PP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use feature dims discovered from pretrain_dataset\n",
        "dapper_dim   = pretrain_dataset.dapper_dim\n",
        "affectnet_dim = pretrain_dataset.affectnet_dim\n",
        "\n",
        "# Init encoders + model\n",
        "physio_encoder = PhysiologicalEncoder(dapper_dim, embed_dim=128)\n",
        "image_encoder  = ImageEncoder(affectnet_dim, embed_dim=128)\n",
        "pretrain_model = PretrainModel(physio_encoder, image_encoder)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrain_model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(pretrain_model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "_CSMoM_FNJHn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5  # increase later\n",
        "for epoch in range(num_epochs):\n",
        "    pretrain_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for physio_batch, img_batch in pretrain_loader:\n",
        "        physio_batch = physio_batch.to(device)\n",
        "        img_batch    = img_batch.to(device)\n",
        "\n",
        "        z_physio, z_image = pretrain_model(physio_batch, img_batch)\n",
        "        loss = contrastive_loss(z_physio, z_image)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Avg Contrastive Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save encoders for fine-tuning\n",
        "torch.save(pretrain_model.physio_encoder.state_dict(), \"physio_encoder.pt\")\n",
        "torch.save(pretrain_model.image_encoder.state_dict(), \"image_encoder.pt\")\n",
        "\n",
        "print(\"✅ Pretraining finished. Encoders saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvwgeogTFCWN",
        "outputId": "7e40e92c-9e7e-44fc-c9df-ccd33fb99a1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Avg Contrastive Loss: 4.1687\n",
            "Epoch 2/5 | Avg Contrastive Loss: 4.1340\n",
            "Epoch 3/5 | Avg Contrastive Loss: 4.1173\n",
            "Epoch 4/5 | Avg Contrastive Loss: 4.1078\n",
            "Epoch 5/5 | Avg Contrastive Loss: 4.1045\n",
            "✅ Pretraining finished. Encoders saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Stage 2 – Fine-tuning with Labels\n",
        "# =========================\n",
        "# Load labelled data\n",
        "dapper_df = pd.read_parquet(\"val.parquet\")\n",
        "affectnet_df = pd.read_csv(\"classifier_dataset.csv\")\n",
        "\n",
        "print(\"DAPPER (labelled) shape:\", dapper_df.shape)\n",
        "print(\"AffectNet (labelled) shape:\", affectnet_df.shape)\n"
      ],
      "metadata": {
        "id": "jLBgden8j29F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e8ef36-6ae3-4044-83d7-59d1f007728b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAPPER (labelled) shape: (100620, 63)\n",
            "AffectNet (labelled) shape: (34, 718)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- DAPPER Dataset ----------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "class DapperDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        # Features\n",
        "        self.X = df.drop(columns=[\"valence\",\"arousal\",\"panas_pos\",\"panas_neg\"], errors=\"ignore\").values.astype(\"float32\")\n",
        "\n",
        "        # Standardize features\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.X = self.scaler_X.fit_transform(self.X)\n",
        "\n",
        "        # Labels\n",
        "        self.y_va = df[[\"valence\",\"arousal\"]].values.astype(\"float32\")\n",
        "        self.y_panas = df[[\"panas_pos\",\"panas_neg\"]].values.astype(\"float32\")\n",
        "\n",
        "        # Standardize labels\n",
        "        self.scaler_va = StandardScaler()\n",
        "        self.y_va = self.scaler_va.fit_transform(self.y_va)\n",
        "\n",
        "        self.scaler_panas = StandardScaler()\n",
        "        self.y_panas = self.scaler_panas.fit_transform(self.y_panas)\n",
        "\n",
        "        # Replace any remaining NaN or Inf\n",
        "        self.X = np.nan_to_num(self.X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_va = np.nan_to_num(self.y_va, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_panas = np.nan_to_num(self.y_panas, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        labels = {\n",
        "            \"va\": torch.tensor(self.y_va[idx], dtype=torch.float32),\n",
        "            \"panas\": torch.tensor(self.y_panas[idx], dtype=torch.float32)\n",
        "        }\n",
        "        return x, labels\n",
        "\n",
        "# ---------------------- AffectNet Dataset ----------------------\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, df, target_dim=709):\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Features\n",
        "        self.X = df.drop(columns=[\"frame\",\"face_id\",\"timestamp\",\"emotion\",\"stress\",\"valence\",\"arousal\"], errors=\"ignore\").values.astype(\"float32\")\n",
        "\n",
        "        # Standardize features\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.X = self.scaler_X.fit_transform(self.X)\n",
        "\n",
        "        # Pad / truncate to match target_dim\n",
        "        if self.X.shape[1] > target_dim:\n",
        "            self.X = self.X[:, :target_dim]\n",
        "        elif self.X.shape[1] < target_dim:\n",
        "            pad_width = target_dim - self.X.shape[1]\n",
        "            self.X = np.pad(self.X, ((0,0),(0,pad_width)), mode=\"constant\")\n",
        "\n",
        "        # Labels\n",
        "        self.y_emotion = self.label_encoder.fit_transform(df[\"emotion\"]).astype(\"int64\")\n",
        "        self.y_va = df[[\"valence\",\"arousal\"]].values.astype(\"float32\")\n",
        "\n",
        "        # Standardize VA labels\n",
        "        self.scaler_va = StandardScaler()\n",
        "        self.y_va = self.scaler_va.fit_transform(self.y_va)\n",
        "\n",
        "        # Replace any remaining NaN / Inf\n",
        "        self.X = np.nan_to_num(self.X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_va = np.nan_to_num(self.y_va, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        labels = {\n",
        "            \"emotion\": torch.tensor(self.y_emotion[idx], dtype=torch.long),\n",
        "            \"va\": torch.tensor(self.y_va[idx], dtype=torch.float32),\n",
        "            \"panas\": torch.zeros(2, dtype=torch.float32)  # dummy PANAS\n",
        "        }\n",
        "        return x, labels\n"
      ],
      "metadata": {
        "id": "IVcM-vAaj6Fm"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dapper = DapperDataset(dapper_df)\n",
        "print(\"VA stats:\", dapper.y_va.min(), dapper.y_va.max(), np.mean(dapper.y_va))\n",
        "print(\"PANAS stats:\", dapper.y_panas.min(), dapper.y_panas.max(), np.mean(dapper.y_panas))\n",
        "\n",
        "affectnet = AffectNetDataset(affectnet_df)\n",
        "print(\"VA stats (AffectNet):\", affectnet.y_va.min(), affectnet.y_va.max(), np.mean(affectnet.y_va))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta7WzrhWoXXW",
        "outputId": "6a9e5530-4e97-4ed5-9a61-f338d458c903"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA stats: -2.8387682 1.9938319 6.55118e-08\n",
            "PANAS stats: -1.9258921 1.8173043 -1.9410903e-08\n",
            "VA stats (AffectNet): -1.2135599 2.8 -4.2073868e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dapper columns:\", dapper_df.columns.tolist())\n",
        "print(\"AffectNet columns:\", affectnet_df.columns.tolist())\n",
        "\n",
        "affectnet_dataset = AffectNetDataset(affectnet_df)\n",
        "print(\"Emotion classes:\", affectnet_dataset.label_encoder.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJnhZ4AAlMqO",
        "outputId": "5a58f90c-72c3-41b6-b11b-9fde5fb87681"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dapper columns: ['participant_id', 'window_id', 'start_time', 'center_time', 'hr_mean', 'hr_std', 'hr_skew', 'hr_kurtosis', 'hr_median', 'hr_iqr', 'hr_min', 'hr_max', 'hr_pwr_total', 'hr_pwr_band_1', 'hr_pwr_band_2', 'gsr_mean', 'gsr_std', 'gsr_skew', 'gsr_kurtosis', 'gsr_median', 'gsr_iqr', 'gsr_min', 'gsr_max', 'gsr_pwr_total', 'gsr_pwr_band_1', 'gsr_pwr_band_2', 'acc_x_mean', 'acc_x_std', 'acc_x_skew', 'acc_x_kurtosis', 'acc_x_median', 'acc_x_iqr', 'acc_x_min', 'acc_x_max', 'acc_x_pwr_total', 'acc_x_pwr_band_1', 'acc_x_pwr_band_2', 'acc_y_mean', 'acc_y_std', 'acc_y_skew', 'acc_y_kurtosis', 'acc_y_median', 'acc_y_iqr', 'acc_y_min', 'acc_y_max', 'acc_y_pwr_total', 'acc_y_pwr_band_1', 'acc_y_pwr_band_2', 'acc_z_mean', 'acc_z_std', 'acc_z_skew', 'acc_z_kurtosis', 'acc_z_median', 'acc_z_iqr', 'acc_z_min', 'acc_z_max', 'acc_z_pwr_total', 'acc_z_pwr_band_1', 'acc_z_pwr_band_2', 'valence', 'arousal', 'panas_pos', 'panas_neg']\n",
            "AffectNet columns: ['frame', 'face_id', 'timestamp', 'confidence', 'success', 'gaze_0_x', 'gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z', 'gaze_angle_x', 'gaze_angle_y', 'eye_lmk_x_0', 'eye_lmk_x_1', 'eye_lmk_x_2', 'eye_lmk_x_3', 'eye_lmk_x_4', 'eye_lmk_x_5', 'eye_lmk_x_6', 'eye_lmk_x_7', 'eye_lmk_x_8', 'eye_lmk_x_9', 'eye_lmk_x_10', 'eye_lmk_x_11', 'eye_lmk_x_12', 'eye_lmk_x_13', 'eye_lmk_x_14', 'eye_lmk_x_15', 'eye_lmk_x_16', 'eye_lmk_x_17', 'eye_lmk_x_18', 'eye_lmk_x_19', 'eye_lmk_x_20', 'eye_lmk_x_21', 'eye_lmk_x_22', 'eye_lmk_x_23', 'eye_lmk_x_24', 'eye_lmk_x_25', 'eye_lmk_x_26', 'eye_lmk_x_27', 'eye_lmk_x_28', 'eye_lmk_x_29', 'eye_lmk_x_30', 'eye_lmk_x_31', 'eye_lmk_x_32', 'eye_lmk_x_33', 'eye_lmk_x_34', 'eye_lmk_x_35', 'eye_lmk_x_36', 'eye_lmk_x_37', 'eye_lmk_x_38', 'eye_lmk_x_39', 'eye_lmk_x_40', 'eye_lmk_x_41', 'eye_lmk_x_42', 'eye_lmk_x_43', 'eye_lmk_x_44', 'eye_lmk_x_45', 'eye_lmk_x_46', 'eye_lmk_x_47', 'eye_lmk_x_48', 'eye_lmk_x_49', 'eye_lmk_x_50', 'eye_lmk_x_51', 'eye_lmk_x_52', 'eye_lmk_x_53', 'eye_lmk_x_54', 'eye_lmk_x_55', 'eye_lmk_y_0', 'eye_lmk_y_1', 'eye_lmk_y_2', 'eye_lmk_y_3', 'eye_lmk_y_4', 'eye_lmk_y_5', 'eye_lmk_y_6', 'eye_lmk_y_7', 'eye_lmk_y_8', 'eye_lmk_y_9', 'eye_lmk_y_10', 'eye_lmk_y_11', 'eye_lmk_y_12', 'eye_lmk_y_13', 'eye_lmk_y_14', 'eye_lmk_y_15', 'eye_lmk_y_16', 'eye_lmk_y_17', 'eye_lmk_y_18', 'eye_lmk_y_19', 'eye_lmk_y_20', 'eye_lmk_y_21', 'eye_lmk_y_22', 'eye_lmk_y_23', 'eye_lmk_y_24', 'eye_lmk_y_25', 'eye_lmk_y_26', 'eye_lmk_y_27', 'eye_lmk_y_28', 'eye_lmk_y_29', 'eye_lmk_y_30', 'eye_lmk_y_31', 'eye_lmk_y_32', 'eye_lmk_y_33', 'eye_lmk_y_34', 'eye_lmk_y_35', 'eye_lmk_y_36', 'eye_lmk_y_37', 'eye_lmk_y_38', 'eye_lmk_y_39', 'eye_lmk_y_40', 'eye_lmk_y_41', 'eye_lmk_y_42', 'eye_lmk_y_43', 'eye_lmk_y_44', 'eye_lmk_y_45', 'eye_lmk_y_46', 'eye_lmk_y_47', 'eye_lmk_y_48', 'eye_lmk_y_49', 'eye_lmk_y_50', 'eye_lmk_y_51', 'eye_lmk_y_52', 'eye_lmk_y_53', 'eye_lmk_y_54', 'eye_lmk_y_55', 'eye_lmk_X_0', 'eye_lmk_X_1', 'eye_lmk_X_2', 'eye_lmk_X_3', 'eye_lmk_X_4', 'eye_lmk_X_5', 'eye_lmk_X_6', 'eye_lmk_X_7', 'eye_lmk_X_8', 'eye_lmk_X_9', 'eye_lmk_X_10', 'eye_lmk_X_11', 'eye_lmk_X_12', 'eye_lmk_X_13', 'eye_lmk_X_14', 'eye_lmk_X_15', 'eye_lmk_X_16', 'eye_lmk_X_17', 'eye_lmk_X_18', 'eye_lmk_X_19', 'eye_lmk_X_20', 'eye_lmk_X_21', 'eye_lmk_X_22', 'eye_lmk_X_23', 'eye_lmk_X_24', 'eye_lmk_X_25', 'eye_lmk_X_26', 'eye_lmk_X_27', 'eye_lmk_X_28', 'eye_lmk_X_29', 'eye_lmk_X_30', 'eye_lmk_X_31', 'eye_lmk_X_32', 'eye_lmk_X_33', 'eye_lmk_X_34', 'eye_lmk_X_35', 'eye_lmk_X_36', 'eye_lmk_X_37', 'eye_lmk_X_38', 'eye_lmk_X_39', 'eye_lmk_X_40', 'eye_lmk_X_41', 'eye_lmk_X_42', 'eye_lmk_X_43', 'eye_lmk_X_44', 'eye_lmk_X_45', 'eye_lmk_X_46', 'eye_lmk_X_47', 'eye_lmk_X_48', 'eye_lmk_X_49', 'eye_lmk_X_50', 'eye_lmk_X_51', 'eye_lmk_X_52', 'eye_lmk_X_53', 'eye_lmk_X_54', 'eye_lmk_X_55', 'eye_lmk_Y_0', 'eye_lmk_Y_1', 'eye_lmk_Y_2', 'eye_lmk_Y_3', 'eye_lmk_Y_4', 'eye_lmk_Y_5', 'eye_lmk_Y_6', 'eye_lmk_Y_7', 'eye_lmk_Y_8', 'eye_lmk_Y_9', 'eye_lmk_Y_10', 'eye_lmk_Y_11', 'eye_lmk_Y_12', 'eye_lmk_Y_13', 'eye_lmk_Y_14', 'eye_lmk_Y_15', 'eye_lmk_Y_16', 'eye_lmk_Y_17', 'eye_lmk_Y_18', 'eye_lmk_Y_19', 'eye_lmk_Y_20', 'eye_lmk_Y_21', 'eye_lmk_Y_22', 'eye_lmk_Y_23', 'eye_lmk_Y_24', 'eye_lmk_Y_25', 'eye_lmk_Y_26', 'eye_lmk_Y_27', 'eye_lmk_Y_28', 'eye_lmk_Y_29', 'eye_lmk_Y_30', 'eye_lmk_Y_31', 'eye_lmk_Y_32', 'eye_lmk_Y_33', 'eye_lmk_Y_34', 'eye_lmk_Y_35', 'eye_lmk_Y_36', 'eye_lmk_Y_37', 'eye_lmk_Y_38', 'eye_lmk_Y_39', 'eye_lmk_Y_40', 'eye_lmk_Y_41', 'eye_lmk_Y_42', 'eye_lmk_Y_43', 'eye_lmk_Y_44', 'eye_lmk_Y_45', 'eye_lmk_Y_46', 'eye_lmk_Y_47', 'eye_lmk_Y_48', 'eye_lmk_Y_49', 'eye_lmk_Y_50', 'eye_lmk_Y_51', 'eye_lmk_Y_52', 'eye_lmk_Y_53', 'eye_lmk_Y_54', 'eye_lmk_Y_55', 'eye_lmk_Z_0', 'eye_lmk_Z_1', 'eye_lmk_Z_2', 'eye_lmk_Z_3', 'eye_lmk_Z_4', 'eye_lmk_Z_5', 'eye_lmk_Z_6', 'eye_lmk_Z_7', 'eye_lmk_Z_8', 'eye_lmk_Z_9', 'eye_lmk_Z_10', 'eye_lmk_Z_11', 'eye_lmk_Z_12', 'eye_lmk_Z_13', 'eye_lmk_Z_14', 'eye_lmk_Z_15', 'eye_lmk_Z_16', 'eye_lmk_Z_17', 'eye_lmk_Z_18', 'eye_lmk_Z_19', 'eye_lmk_Z_20', 'eye_lmk_Z_21', 'eye_lmk_Z_22', 'eye_lmk_Z_23', 'eye_lmk_Z_24', 'eye_lmk_Z_25', 'eye_lmk_Z_26', 'eye_lmk_Z_27', 'eye_lmk_Z_28', 'eye_lmk_Z_29', 'eye_lmk_Z_30', 'eye_lmk_Z_31', 'eye_lmk_Z_32', 'eye_lmk_Z_33', 'eye_lmk_Z_34', 'eye_lmk_Z_35', 'eye_lmk_Z_36', 'eye_lmk_Z_37', 'eye_lmk_Z_38', 'eye_lmk_Z_39', 'eye_lmk_Z_40', 'eye_lmk_Z_41', 'eye_lmk_Z_42', 'eye_lmk_Z_43', 'eye_lmk_Z_44', 'eye_lmk_Z_45', 'eye_lmk_Z_46', 'eye_lmk_Z_47', 'eye_lmk_Z_48', 'eye_lmk_Z_49', 'eye_lmk_Z_50', 'eye_lmk_Z_51', 'eye_lmk_Z_52', 'eye_lmk_Z_53', 'eye_lmk_Z_54', 'eye_lmk_Z_55', 'pose_Tx', 'pose_Ty', 'pose_Tz', 'pose_Rx', 'pose_Ry', 'pose_Rz', 'x_0', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_11', 'x_12', 'x_13', 'x_14', 'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_21', 'x_22', 'x_23', 'x_24', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58', 'x_59', 'x_60', 'x_61', 'x_62', 'x_63', 'x_64', 'x_65', 'x_66', 'x_67', 'y_0', 'y_1', 'y_2', 'y_3', 'y_4', 'y_5', 'y_6', 'y_7', 'y_8', 'y_9', 'y_10', 'y_11', 'y_12', 'y_13', 'y_14', 'y_15', 'y_16', 'y_17', 'y_18', 'y_19', 'y_20', 'y_21', 'y_22', 'y_23', 'y_24', 'y_25', 'y_26', 'y_27', 'y_28', 'y_29', 'y_30', 'y_31', 'y_32', 'y_33', 'y_34', 'y_35', 'y_36', 'y_37', 'y_38', 'y_39', 'y_40', 'y_41', 'y_42', 'y_43', 'y_44', 'y_45', 'y_46', 'y_47', 'y_48', 'y_49', 'y_50', 'y_51', 'y_52', 'y_53', 'y_54', 'y_55', 'y_56', 'y_57', 'y_58', 'y_59', 'y_60', 'y_61', 'y_62', 'y_63', 'y_64', 'y_65', 'y_66', 'y_67', 'X_0', 'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18', 'X_19', 'X_20', 'X_21', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28', 'X_29', 'X_30', 'X_31', 'X_32', 'X_33', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_39', 'X_40', 'X_41', 'X_42', 'X_43', 'X_44', 'X_45', 'X_46', 'X_47', 'X_48', 'X_49', 'X_50', 'X_51', 'X_52', 'X_53', 'X_54', 'X_55', 'X_56', 'X_57', 'X_58', 'X_59', 'X_60', 'X_61', 'X_62', 'X_63', 'X_64', 'X_65', 'X_66', 'X_67', 'Y_0', 'Y_1', 'Y_2', 'Y_3', 'Y_4', 'Y_5', 'Y_6', 'Y_7', 'Y_8', 'Y_9', 'Y_10', 'Y_11', 'Y_12', 'Y_13', 'Y_14', 'Y_15', 'Y_16', 'Y_17', 'Y_18', 'Y_19', 'Y_20', 'Y_21', 'Y_22', 'Y_23', 'Y_24', 'Y_25', 'Y_26', 'Y_27', 'Y_28', 'Y_29', 'Y_30', 'Y_31', 'Y_32', 'Y_33', 'Y_34', 'Y_35', 'Y_36', 'Y_37', 'Y_38', 'Y_39', 'Y_40', 'Y_41', 'Y_42', 'Y_43', 'Y_44', 'Y_45', 'Y_46', 'Y_47', 'Y_48', 'Y_49', 'Y_50', 'Y_51', 'Y_52', 'Y_53', 'Y_54', 'Y_55', 'Y_56', 'Y_57', 'Y_58', 'Y_59', 'Y_60', 'Y_61', 'Y_62', 'Y_63', 'Y_64', 'Y_65', 'Y_66', 'Y_67', 'Z_0', 'Z_1', 'Z_2', 'Z_3', 'Z_4', 'Z_5', 'Z_6', 'Z_7', 'Z_8', 'Z_9', 'Z_10', 'Z_11', 'Z_12', 'Z_13', 'Z_14', 'Z_15', 'Z_16', 'Z_17', 'Z_18', 'Z_19', 'Z_20', 'Z_21', 'Z_22', 'Z_23', 'Z_24', 'Z_25', 'Z_26', 'Z_27', 'Z_28', 'Z_29', 'Z_30', 'Z_31', 'Z_32', 'Z_33', 'Z_34', 'Z_35', 'Z_36', 'Z_37', 'Z_38', 'Z_39', 'Z_40', 'Z_41', 'Z_42', 'Z_43', 'Z_44', 'Z_45', 'Z_46', 'Z_47', 'Z_48', 'Z_49', 'Z_50', 'Z_51', 'Z_52', 'Z_53', 'Z_54', 'Z_55', 'Z_56', 'Z_57', 'Z_58', 'Z_59', 'Z_60', 'Z_61', 'Z_62', 'Z_63', 'Z_64', 'Z_65', 'Z_66', 'Z_67', 'p_scale', 'p_rx', 'p_ry', 'p_rz', 'p_tx', 'p_ty', 'p_0', 'p_1', 'p_2', 'p_3', 'p_4', 'p_5', 'p_6', 'p_7', 'p_8', 'p_9', 'p_10', 'p_11', 'p_12', 'p_13', 'p_14', 'p_15', 'p_16', 'p_17', 'p_18', 'p_19', 'p_20', 'p_21', 'p_22', 'p_23', 'p_24', 'p_25', 'p_26', 'p_27', 'p_28', 'p_29', 'p_30', 'p_31', 'p_32', 'p_33', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r', 'AU01_c', 'AU02_c', 'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c', 'AU09_c', 'AU10_c', 'AU12_c', 'AU14_c', 'AU15_c', 'AU17_c', 'AU20_c', 'AU23_c', 'AU25_c', 'AU26_c', 'AU28_c', 'AU45_c', 'emotion', 'stress', 'valence', 'arousal']\n",
            "Emotion classes: ['angry' 'happy' 'neutral' 'sad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dapper_dataset = DapperDataset(dapper_df)\n",
        "affectnet_dataset = AffectNetDataset(affectnet_df)\n",
        "\n",
        "dapper_loader = DataLoader(dapper_dataset, batch_size=64, shuffle=True)\n",
        "affectnet_loader = DataLoader(affectnet_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "sAOcruLoj9q9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_features(df, drop_cols):\n",
        "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "    df = df.select_dtypes(include=[np.number])\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    df = df.fillna(df.mean()).fillna(0.0)\n",
        "    return df.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "ejdEWkuPkzsc"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean features consistently with pretraining ---\n",
        "dapper_clean = clean_features(\n",
        "    dapper_df,\n",
        "    drop_cols=[\"participant_id\", \"window_id\", \"start_time\", \"center_time\",\n",
        "               \"valence\", \"arousal\", \"panas_pos\", \"panas_neg\"]\n",
        ")\n",
        "affectnet_clean = clean_features(\n",
        "    affectnet_df,\n",
        "    drop_cols=[\"frame\", \"face_id\", \"timestamp\", \"emotion\", \"stress\",\n",
        "               \"valence\", \"arousal\", \"confidence\", \"success\"]\n",
        ")\n",
        "\n",
        "# Get feature dims (must match pretrained encoders)\n",
        "dapper_dim = dapper_clean.shape[1]\n",
        "affectnet_dim = affectnet_clean.shape[1]\n",
        "\n",
        "print(f\"DAPPER dim = {dapper_dim}, AffectNet dim = {affectnet_dim}\")\n",
        "\n",
        "# --- Load encoders ---\n",
        "physio_encoder = PhysiologicalEncoder(dapper_dim)\n",
        "physio_encoder.load_state_dict(torch.load(\"physio_encoder.pt\", map_location=device))\n",
        "physio_encoder.eval()  # eval mode\n",
        "\n",
        "image_encoder = ImageEncoder(affectnet_dim)\n",
        "image_encoder.load_state_dict(torch.load(\"image_encoder.pt\", map_location=device))\n",
        "image_encoder.eval()\n",
        "\n",
        "# --- Multi-task classifier ---\n",
        "class MultiTaskClassifier(nn.Module):\n",
        "    def __init__(self, physio_encoder, image_encoder, embed_dim=128, num_emotions=7):\n",
        "        super().__init__()\n",
        "        self.physio_encoder = physio_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "\n",
        "        # freeze encoders (no gradient updates)\n",
        "        for p in self.physio_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.image_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.emotion_head = nn.Linear(embed_dim, num_emotions)   # CE loss\n",
        "        self.va_head      = nn.Linear(embed_dim, 2)              # MSE loss\n",
        "        self.panas_head   = nn.Linear(embed_dim, 2)              # MSE loss\n",
        "\n",
        "    def forward(self, x, modality=\"physio\"):\n",
        "        if modality == \"physio\":\n",
        "            z = self.physio_encoder(x)\n",
        "        else:\n",
        "            z = self.image_encoder(x)\n",
        "\n",
        "        return {\n",
        "            \"emotion\": self.emotion_head(z),\n",
        "            \"va\": self.va_head(z),\n",
        "            \"panas\": self.panas_head(z)\n",
        "        }\n",
        "\n",
        "# --- Build model ---\n",
        "model = MultiTaskClassifier(\n",
        "    physio_encoder, image_encoder, embed_dim=128, num_emotions=7\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTBxPIgDjqs-",
        "outputId": "5e1eb025-3957-4532-a136-be7bb242289c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAPPER dim = 55, AffectNet dim = 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Loss Function ----------------------\n",
        "def compute_loss(outputs, labels, modality):\n",
        "    loss = 0.0\n",
        "    if modality == \"image\" and \"emotion\" in labels:\n",
        "        loss += nn.CrossEntropyLoss()(outputs[\"emotion\"], labels[\"emotion\"])\n",
        "    if \"va\" in labels:\n",
        "        loss += nn.MSELoss()(torch.nan_to_num(outputs[\"va\"]), torch.nan_to_num(labels[\"va\"]))\n",
        "    if \"panas\" in labels:\n",
        "        loss += nn.MSELoss()(torch.nan_to_num(outputs[\"panas\"]), torch.nan_to_num(labels[\"panas\"]))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "3I6jryz6jw87"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dapper_loader, affectnet_loader, device, lr=1e-3, num_epochs=5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, steps = 0.0, 0\n",
        "\n",
        "        # --- DAPPER ---\n",
        "        for physio_x, physio_labels in dapper_loader:\n",
        "            physio_x = physio_x.to(device)\n",
        "            labels = {k: v.to(device) for k, v in physio_labels.items()}\n",
        "\n",
        "            outputs = model(physio_x, modality=\"physio\")\n",
        "\n",
        "            # Skip batch if NaN in input or output\n",
        "            if torch.isnan(physio_x).any() or any(torch.isnan(v).any() for v in outputs.values()):\n",
        "                print(\"NaN detected in DAPPER batch – skipping\")\n",
        "                continue\n",
        "\n",
        "            loss = compute_loss(outputs, labels, modality=\"physio\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        # --- AffectNet ---\n",
        "        for image_x, image_labels in affectnet_loader:\n",
        "            image_x = image_x.to(device)\n",
        "            labels = {k: v.to(device) for k, v in image_labels.items()}\n",
        "\n",
        "            outputs = model(image_x, modality=\"image\")\n",
        "\n",
        "            if torch.isnan(image_x).any() or any(torch.isnan(v).any() for v in outputs.values()):\n",
        "                print(\"NaN detected in AffectNet batch – skipping\")\n",
        "                continue\n",
        "\n",
        "            loss = compute_loss(outputs, labels, modality=\"image\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Loss: {total_loss/steps:.4f}\")\n",
        "\n",
        "# Instantiate datasets\n",
        "dapper_dataset = DapperDataset(dapper_df)\n",
        "affectnet_dataset = AffectNetDataset(affectnet_df, target_dim=pretrain_dataset.affectnet_dim)\n",
        "\n",
        "# DataLoaders\n",
        "dapper_loader = DataLoader(dapper_dataset, batch_size=32, shuffle=True)\n",
        "affectnet_loader = DataLoader(affectnet_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Call training\n",
        "train(\n",
        "    model=model,\n",
        "    dapper_loader=dapper_loader,\n",
        "    affectnet_loader=affectnet_loader,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    num_epochs=5\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7vK79sxj0CK",
        "outputId": "98c9b206-2e6e-43c4-bb35-19b570565584"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Avg Loss: 1.7870\n",
            "Epoch 2/5 | Avg Loss: 1.6472\n",
            "Epoch 3/5 | Avg Loss: 1.6016\n",
            "Epoch 4/5 | Avg Loss: 1.5727\n",
            "Epoch 5/5 | Avg Loss: 1.5550\n"
          ]
        }
      ]
    }
  ]
}