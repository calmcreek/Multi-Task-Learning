{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2270qyeF+44kKZBtCOS0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmcreek/Multi-Task-Learning/blob/main/encoders_and_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "BPbDnU-SD9Lc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the dataset files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "CmLeK6KKis6F",
        "outputId": "47b9724a-7c73-47fe-edb0-06853577855b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-761c446f-8fd4-4e5f-b332-43869efa94fb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-761c446f-8fd4-4e5f-b332-43869efa94fb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving val.parquet to val.parquet\n",
            "Saving train.parquet to train.parquet\n",
            "Saving classifier_dataset.csv to classifier_dataset.csv\n",
            "Saving ssl_dataset.csv to ssl_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "==============================================ENCODERS IMPLEMENTATION====================================================\n",
        "Training using unlabelled dataset\n",
        "\"\"\"\n",
        "\n",
        "#AffectNet features (currently in CSV format)\n",
        "affectnet_df = pd.read_csv(\"ssl_dataset.csv\", low_memory=False)\n",
        "\n",
        "#DAPPER physiological features (currently in Parquet format)\n",
        "dapper_df = pd.read_parquet(\"train.parquet\")\n",
        "\n",
        "print(\"AffectNet shape:\", affectnet_df.shape)\n",
        "print(\"DAPPER shape:\", dapper_df.shape)\n",
        "print(\"AffectNet dtypes (sample):\\n\", affectnet_df.dtypes.head())\n",
        "print(\"DAPPER dtypes (sample):\\n\", dapper_df.dtypes.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rZ1neAD-Ts",
        "outputId": "3221d709-217f-4e36-cbfc-badaeb158cba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AffectNet shape: (121, 718)\n",
            "DAPPER shape: (341248, 63)\n",
            "AffectNet dtypes (sample):\n",
            " frame           int64\n",
            "face_id         int64\n",
            "timestamp     float64\n",
            "confidence    float64\n",
            "success         int64\n",
            "dtype: object\n",
            "DAPPER dtypes (sample):\n",
            " participant_id      int64\n",
            "window_id           int64\n",
            "start_time          int64\n",
            "center_time       float64\n",
            "hr_mean           float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PretrainDataset pairs physiological features (from DAPPER) and facial/image features (from AffectNet)\n",
        "for self-supervised pretraining. It:\n",
        "- Cleans both datasets by dropping IDs/labels, non-numeric columns, and all-NaN columns.\n",
        "- Fills missing values with column means (or 0 if still NaN).\n",
        "- Converts data to float32 tensors.\n",
        "- Truncates both datasets to the same length for paired sampling.\n",
        "Each item returns a tuple: (physio_tensor, image_features_tensor).\n",
        "\"\"\"\n",
        "\n",
        "class PretrainDataset(Dataset):\n",
        "    def __init__(self, dapper_df, affectnet_df):\n",
        "        #Working on copies, to avoid modifying original dfs\n",
        "        dapper = dapper_df.copy()\n",
        "        affectnet = affectnet_df.copy()\n",
        "\n",
        "        #Dropping known label/id columns (if present)\n",
        "        dapper = dapper.drop(columns=[\"participant_id\", \"window_id\", \"start_time\", \"center_time\", \"valence\", \"arousal\", \"panas_pos\", \"panas_neg\"], errors=\"ignore\")\n",
        "        affectnet = affectnet.drop(columns=[\"frame\", \"face_id\", \"timestamp\", \"emotion\", \"stress\", \"valence\", \"arousal\", \"confidence\", \"success\"], errors=\"ignore\")\n",
        "\n",
        "        #Keeping only numeric columns\n",
        "        dapper = dapper.select_dtypes(include=[np.number])\n",
        "        affectnet = affectnet.select_dtypes(include=[np.number])\n",
        "\n",
        "        #Dropping columns that are entirely NaN\n",
        "        dapper = dapper.dropna(axis=1, how=\"all\")\n",
        "        affectnet = affectnet.dropna(axis=1, how=\"all\")\n",
        "\n",
        "        #Filling remaining NaNs\n",
        "        dapper = dapper.fillna(dapper.mean()).fillna(0.0)\n",
        "        affectnet = affectnet.fillna(affectnet.mean()).fillna(0.0)\n",
        "\n",
        "        #Converting to float32\n",
        "        self.dapper = dapper.values.astype(np.float32)\n",
        "        self.affectnet = affectnet.values.astype(np.float32)\n",
        "\n",
        "        #Keeping feature dims for later use\n",
        "        self.dapper_dim = self.dapper.shape[1]\n",
        "        self.affectnet_dim = self.affectnet.shape[1]\n",
        "\n",
        "        #Align lengths: cut to the shorter dataset (simple strategy for paired SSL)\n",
        "        min_len = min(len(self.dapper), len(self.affectnet))\n",
        "        self.dapper = self.dapper[:min_len]\n",
        "        self.affectnet = self.affectnet[:min_len]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dapper)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #returns (physio_tensor, image_features_tensor)\n",
        "        d = torch.from_numpy(self.dapper[idx])\n",
        "        a = torch.from_numpy(self.affectnet[idx])\n",
        "        return d, a\n",
        "\n",
        "#Instantiate loader\n",
        "pretrain_dataset = PretrainDataset(dapper_df, affectnet_df)\n",
        "pretrain_loader = DataLoader(pretrain_dataset, batch_size=64, shuffle=True, drop_last=False)\n",
        "\n",
        "#Sanity prints\n",
        "print(\"Pretrain dataset length:\", len(pretrain_dataset))\n",
        "print(\"DAPPER feature dim:\", pretrain_dataset.dapper_dim)\n",
        "print(\"AffectNet feature dim:\", pretrain_dataset.affectnet_dim)\n"
      ],
      "metadata": {
        "id": "Y77XIz4dEM_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb398030-8b23-4436-d603-c3bc4e03e164"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain dataset length: 121\n",
            "DAPPER feature dim: 55\n",
            "AffectNet feature dim: 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PhysiologicalEncoder maps physiological feature vectors into a fixed-size embedding.\n",
        "- Treats each feature vector as a 1D signal with one channel.\n",
        "- Uses stacked Conv1D, BatchNorm, ReLU, and pooling layers to extract representations.\n",
        "- Applies AdaptiveAvgPool to collapse temporal dimension, then a Linear layer to project into embed_dim.\n",
        "Input: (batch, features) → Output: (batch, embed_dim).\n",
        "\"\"\"\n",
        "\n",
        "class PhysiologicalEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128):\n",
        "        \"\"\"\n",
        "        input_dim: number of features per window (treated as sequence length for 1D conv)\n",
        "        embed_dim: final embedding size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #Treating the features as a 1D 'signal' of length=input_dim with 1 channel\n",
        "        self.input_dim = input_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),  #halves length\n",
        "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)  #collapse length -> 1\n",
        "        )\n",
        "        self.fc = nn.Linear(64, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_dim) float tensor\n",
        "        returns: (batch, embed_dim)\n",
        "        \"\"\"\n",
        "        #ensure float and correct shape\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(\"Physio encoder expects input shape (batch, features)\")\n",
        "        x = x.unsqueeze(1)  #(batch, 1, features)\n",
        "        x = self.encoder(x).squeeze(-1)  #(batch, 64)\n",
        "        out = self.fc(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Hsr0PxidEPaS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ImageEncoder maps precomputed AffectNet image feature vectors into a fixed-size embedding.\n",
        "- Implements a simple MLP with one hidden layer (512 units), ReLU activation, and dropout.\n",
        "- Projects input_dim → 512 → embed_dim for use in a shared embedding space.\n",
        "Input: (batch, input_dim) → Output: (batch, embed_dim).\n",
        "\"\"\"\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim=128):\n",
        "        \"\"\"\n",
        "        A simple MLP encoder for precomputed image features (AffectNet row features)\n",
        "        input_dim: number of feature columns from affectnet after preprocessing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, input_dim)\n",
        "        return self.encoder(x)\n"
      ],
      "metadata": {
        "id": "myXPjO9FERuk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Contrastive Pretraining (InfoNCE loss)\n",
        "\"\"\"\n",
        "def contrastive_loss(z_physio, z_image, temperature=0.1):\n",
        "    #Normalizing embeddings\n",
        "    z_physio = F.normalize(z_physio, dim=1)\n",
        "    z_image  = F.normalize(z_image, dim=1)\n",
        "\n",
        "    #Similarity matrix\n",
        "    logits = torch.matmul(z_physio, z_image.T) / temperature\n",
        "    labels = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "    #Loss physio->image\n",
        "    loss_i = F.cross_entropy(logits, labels)\n",
        "\n",
        "    #Loss image->physio (transpose logits)\n",
        "    loss_t = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    return (loss_i + loss_t) / 2.0\n"
      ],
      "metadata": {
        "id": "jbdqSuIXE-FD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainModel(nn.Module):\n",
        "    def __init__(self, physio_encoder, image_encoder):\n",
        "        super().__init__()\n",
        "        self.physio_encoder = physio_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "\n",
        "    def forward(self, physio_x, image_x):\n",
        "        return self.physio_encoder(physio_x), self.image_encoder(image_x)\n"
      ],
      "metadata": {
        "id": "WQGteQA-E_PP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using feature dims, discovered from pretrain_dataset\n",
        "dapper_dim   = pretrain_dataset.dapper_dim\n",
        "affectnet_dim = pretrain_dataset.affectnet_dim\n",
        "\n",
        "#Initialize encoders + model\n",
        "physio_encoder = PhysiologicalEncoder(dapper_dim, embed_dim=128)\n",
        "image_encoder  = ImageEncoder(affectnet_dim, embed_dim=128)\n",
        "pretrain_model = PretrainModel(physio_encoder, image_encoder)\n",
        "\n",
        "#Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrain_model.to(device)\n",
        "\n",
        "#Optimizer\n",
        "optimizer = optim.Adam(pretrain_model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "_CSMoM_FNJHn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Training loop\n",
        "num_epochs = 5  #randomly set to 5, can be increased later\n",
        "for epoch in range(num_epochs):\n",
        "    pretrain_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for physio_batch, img_batch in pretrain_loader:\n",
        "        physio_batch = physio_batch.to(device)\n",
        "        img_batch    = img_batch.to(device)\n",
        "\n",
        "        z_physio, z_image = pretrain_model(physio_batch, img_batch)\n",
        "        loss = contrastive_loss(z_physio, z_image)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() #Back-propagation\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Avg Contrastive Loss: {avg_loss:.4f}\")\n",
        "\n",
        "#Save the encoders for fine-tuning\n",
        "torch.save(pretrain_model.physio_encoder.state_dict(), \"physio_encoder.pt\")\n",
        "torch.save(pretrain_model.image_encoder.state_dict(), \"image_encoder.pt\")\n",
        "\n",
        "print(\"✅ Pretraining finished. Encoders saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvwgeogTFCWN",
        "outputId": "acde583b-2bff-4120-d77e-e15e2e8877b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Avg Contrastive Loss: 4.1711\n",
            "Epoch 2/5 | Avg Contrastive Loss: 4.1361\n",
            "Epoch 3/5 | Avg Contrastive Loss: 4.1243\n",
            "Epoch 4/5 | Avg Contrastive Loss: 4.1143\n",
            "Epoch 5/5 | Avg Contrastive Loss: 4.1103\n",
            "✅ Pretraining finished. Encoders saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "===========================================CLASSIFIER IMPLEMENTATION=====================================================\n",
        "Fine-tuning with Labels\n",
        "\"\"\"\n",
        "#Loading the labelled datasets\n",
        "dapper_df = pd.read_parquet(\"val.parquet\")\n",
        "affectnet_df = pd.read_csv(\"classifier_dataset.csv\")\n",
        "\n",
        "print(\"DAPPER (labelled) shape:\", dapper_df.shape)\n",
        "print(\"AffectNet (labelled) shape:\", affectnet_df.shape)\n"
      ],
      "metadata": {
        "id": "jLBgden8j29F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b234a0-4969-4c72-909e-14c7f715c42e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAPPER (labelled) shape: (100620, 63)\n",
            "AffectNet (labelled) shape: (34, 718)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- DAPPER Dataset ----------------------\n",
        "class DapperDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        #Features\n",
        "        self.X = df.drop(columns=[\"valence\",\"arousal\",\"panas_pos\",\"panas_neg\"], errors=\"ignore\").values.astype(\"float32\")\n",
        "\n",
        "        #Standardizing the features\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.X = self.scaler_X.fit_transform(self.X)\n",
        "\n",
        "        #Labels\n",
        "        self.y_va = df[[\"valence\",\"arousal\"]].values.astype(\"float32\")\n",
        "        self.y_panas = df[[\"panas_pos\",\"panas_neg\"]].values.astype(\"float32\")\n",
        "\n",
        "        #Standardizing the labels\n",
        "        self.scaler_va = StandardScaler()\n",
        "        self.y_va = self.scaler_va.fit_transform(self.y_va)\n",
        "\n",
        "        self.scaler_panas = StandardScaler()\n",
        "        self.y_panas = self.scaler_panas.fit_transform(self.y_panas)\n",
        "\n",
        "        #Replacement of any remaining NaN or Inf\n",
        "        self.X = np.nan_to_num(self.X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_va = np.nan_to_num(self.y_va, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_panas = np.nan_to_num(self.y_panas, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        labels = {\n",
        "            \"va\": torch.tensor(self.y_va[idx], dtype=torch.float32),\n",
        "            \"panas\": torch.tensor(self.y_panas[idx], dtype=torch.float32)\n",
        "        }\n",
        "        return x, labels\n",
        "\n",
        "# ---------------------- AffectNet Dataset ----------------------\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, df, target_dim=709):\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        #Features\n",
        "        self.X = df.drop(columns=[\"frame\",\"face_id\",\"timestamp\",\"emotion\",\"stress\",\"valence\",\"arousal\"], errors=\"ignore\").values.astype(\"float32\")\n",
        "\n",
        "        #Standardizing the features\n",
        "        self.scaler_X = StandardScaler()\n",
        "        self.X = self.scaler_X.fit_transform(self.X)\n",
        "\n",
        "        #Truncating to match target_dim\n",
        "        if self.X.shape[1] > target_dim:\n",
        "            self.X = self.X[:, :target_dim]\n",
        "        elif self.X.shape[1] < target_dim:\n",
        "            pad_width = target_dim - self.X.shape[1]\n",
        "            self.X = np.pad(self.X, ((0,0),(0,pad_width)), mode=\"constant\")\n",
        "\n",
        "        #Labels\n",
        "        self.y_emotion = self.label_encoder.fit_transform(df[\"emotion\"]).astype(\"int64\")\n",
        "        self.y_va = df[[\"valence\",\"arousal\"]].values.astype(\"float32\")\n",
        "\n",
        "        #Standardizing VA labels\n",
        "        self.scaler_va = StandardScaler()\n",
        "        self.y_va = self.scaler_va.fit_transform(self.y_va)\n",
        "\n",
        "        #Replacement of any remaining NaN / Inf\n",
        "        self.X = np.nan_to_num(self.X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        self.y_va = np.nan_to_num(self.y_va, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        labels = {\n",
        "            \"emotion\": torch.tensor(self.y_emotion[idx], dtype=torch.long),\n",
        "            \"va\": torch.tensor(self.y_va[idx], dtype=torch.float32),\n",
        "            \"panas\": torch.zeros(2, dtype=torch.float32)  # dummy PANAS\n",
        "        }\n",
        "        return x, labels\n"
      ],
      "metadata": {
        "id": "IVcM-vAaj6Fm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cleans a dataframe by:\n",
        "- Dropping specified columns (if present).\n",
        "- Keeping only numeric columns.\n",
        "- Removing columns that are entirely NaN.\n",
        "- Filling remaining NaNs with column means (or 0 if still NaN).\n",
        "- Converting all values to float32.\n",
        "Returns the cleaned dataframe.\n",
        "\"\"\"\n",
        "\n",
        "def clean_features(df, drop_cols):\n",
        "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "    df = df.select_dtypes(include=[np.number])\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    df = df.fillna(df.mean()).fillna(0.0)\n",
        "    return df.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "ejdEWkuPkzsc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean features consistently with pretraining ---\n",
        "dapper_clean = clean_features(\n",
        "    dapper_df,\n",
        "    drop_cols=[\"participant_id\", \"window_id\", \"start_time\", \"center_time\",\n",
        "               \"valence\", \"arousal\", \"panas_pos\", \"panas_neg\"]\n",
        ")\n",
        "affectnet_clean = clean_features(\n",
        "    affectnet_df,\n",
        "    drop_cols=[\"frame\", \"face_id\", \"timestamp\", \"emotion\", \"stress\",\n",
        "               \"valence\", \"arousal\", \"confidence\", \"success\"]\n",
        ")\n",
        "\n",
        "#Getting the feature dims (must match pretrained encoders)\n",
        "dapper_dim = dapper_clean.shape[1]\n",
        "affectnet_dim = affectnet_clean.shape[1]\n",
        "\n",
        "print(f\"DAPPER dim = {dapper_dim}, AffectNet dim = {affectnet_dim}\")\n",
        "\n",
        "# --- Loading the encoders ---\n",
        "physio_encoder = PhysiologicalEncoder(dapper_dim)\n",
        "physio_encoder.load_state_dict(torch.load(\"physio_encoder.pt\", map_location=device))\n",
        "physio_encoder.eval()  # eval mode\n",
        "\n",
        "image_encoder = ImageEncoder(affectnet_dim)\n",
        "image_encoder.load_state_dict(torch.load(\"image_encoder.pt\", map_location=device))\n",
        "image_encoder.eval()\n",
        "\n",
        "# --- Multi-task classifier ---\n",
        "class MultiTaskClassifier(nn.Module):\n",
        "    def __init__(self, physio_encoder, image_encoder, embed_dim=128, num_emotions=7):\n",
        "        super().__init__()\n",
        "        self.physio_encoder = physio_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "\n",
        "        #freeze the encoders (no gradient updates)\n",
        "        for p in self.physio_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.image_encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        #Implementation of the Task-specific heads\n",
        "        self.emotion_head = nn.Linear(embed_dim, num_emotions)   # CE loss\n",
        "        self.va_head      = nn.Linear(embed_dim, 2)              # MSE loss\n",
        "        self.panas_head   = nn.Linear(embed_dim, 2)              # MSE loss\n",
        "\n",
        "    def forward(self, x, modality=\"physio\"):\n",
        "        if modality == \"physio\":\n",
        "            z = self.physio_encoder(x)\n",
        "        else:\n",
        "            z = self.image_encoder(x)\n",
        "\n",
        "        return {\n",
        "            \"emotion\": self.emotion_head(z),\n",
        "            \"va\": self.va_head(z),\n",
        "            \"panas\": self.panas_head(z)\n",
        "        }\n",
        "\n",
        "# --- Build model ---\n",
        "model = MultiTaskClassifier(\n",
        "    physio_encoder, image_encoder, embed_dim=128, num_emotions=7\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTBxPIgDjqs-",
        "outputId": "251ab676-8398-4d5e-b2c3-7e4f9f56cd76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAPPER dim = 55, AffectNet dim = 709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Computes supervised loss for multimodal predictions based on available labels.\n",
        "- For image modality with emotion labels → uses CrossEntropyLoss.\n",
        "- For valence/arousal (\"va\") and PANAS scores → uses MSELoss with NaNs replaced by zeros.\n",
        "- Accumulates loss terms for all present label types and returns the total loss.\n",
        "\"\"\"\n",
        "\n",
        "def compute_loss(outputs, labels, modality):\n",
        "    loss = 0.0\n",
        "    if modality == \"image\" and \"emotion\" in labels:\n",
        "        loss += nn.CrossEntropyLoss()(outputs[\"emotion\"], labels[\"emotion\"])\n",
        "    if \"va\" in labels:\n",
        "        loss += nn.MSELoss()(torch.nan_to_num(outputs[\"va\"]), torch.nan_to_num(labels[\"va\"]))\n",
        "    if \"panas\" in labels:\n",
        "        loss += nn.MSELoss()(torch.nan_to_num(outputs[\"panas\"]), torch.nan_to_num(labels[\"panas\"]))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "3I6jryz6jw87"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Trains a multimodal model on DAPPER (physiological) and AffectNet (image) data.\n",
        "- Iterates through epochs, alternating between DAPPER and AffectNet loaders.\n",
        "- Moves inputs and labels to device, performs forward pass, and computes loss.\n",
        "- Skips batches with NaNs in inputs or outputs.\n",
        "- Uses backpropagation with gradient clipping and Adam optimizer updates.\n",
        "- Logs average loss per epoch.\n",
        "\"\"\"\n",
        "\n",
        "def train(model, dapper_loader, affectnet_loader, device, lr=1e-3, num_epochs=5):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, steps = 0.0, 0\n",
        "\n",
        "        # --- DAPPER ---\n",
        "        for physio_x, physio_labels in dapper_loader:\n",
        "            physio_x = physio_x.to(device)\n",
        "            labels = {k: v.to(device) for k, v in physio_labels.items()}\n",
        "\n",
        "            outputs = model(physio_x, modality=\"physio\")\n",
        "\n",
        "            #Skip batch if NaN in input or output\n",
        "            if torch.isnan(physio_x).any() or any(torch.isnan(v).any() for v in outputs.values()):\n",
        "                print(\"NaN detected in DAPPER batch – skipping\")\n",
        "                continue\n",
        "\n",
        "            loss = compute_loss(outputs, labels, modality=\"physio\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward() #Back-propagation\n",
        "\n",
        "            #Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        # --- AffectNet ---\n",
        "        for image_x, image_labels in affectnet_loader:\n",
        "            image_x = image_x.to(device)\n",
        "            labels = {k: v.to(device) for k, v in image_labels.items()}\n",
        "\n",
        "            outputs = model(image_x, modality=\"image\")\n",
        "\n",
        "            if torch.isnan(image_x).any() or any(torch.isnan(v).any() for v in outputs.values()):\n",
        "                print(\"NaN detected in AffectNet batch – skipping\")\n",
        "                continue\n",
        "\n",
        "            loss = compute_loss(outputs, labels, modality=\"image\")\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Loss: {total_loss/steps:.4f}\")\n"
      ],
      "metadata": {
        "id": "Y7vK79sxj0CK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate datasets\n",
        "dapper_dataset = DapperDataset(dapper_df)\n",
        "affectnet_dataset = AffectNetDataset(affectnet_df, target_dim=pretrain_dataset.affectnet_dim)\n",
        "\n",
        "#DataLoaders\n",
        "dapper_loader = DataLoader(dapper_dataset, batch_size=32, shuffle=True)\n",
        "affectnet_loader = DataLoader(affectnet_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "#Moving the model to device\n",
        "model.to(device)\n",
        "\n",
        "#Training\n",
        "train(\n",
        "    model=model,\n",
        "    dapper_loader=dapper_loader,\n",
        "    affectnet_loader=affectnet_loader,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    num_epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWaVREMcqw7c",
        "outputId": "daaa4350-ff70-41cc-8903-972c5ce813af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Avg Loss: 1.8012\n",
            "Epoch 2/5 | Avg Loss: 1.6499\n",
            "Epoch 3/5 | Avg Loss: 1.5863\n",
            "Epoch 4/5 | Avg Loss: 1.5486\n",
            "Epoch 5/5 | Avg Loss: 1.5305\n"
          ]
        }
      ]
    }
  ]
}